### 什么是hadoop
狭义上来说,hadoop是一个分布式的软件框架,利用服务器集群对海量数据进行分布式处理
广义上来说,是一个生态圈

### HDFS是什么
1. Hdfs是一个分布式存储管理系统 
2. 优点是可以进行海量存储,而且也有高容错性,表现在数据丢失后还有副本,在大文件存储时能切割写入多次读取
3. 缺点是 不能做到低延迟数据的访问,也不太适合很多小文件的存储

---

### HDFS的组件
1. Namenode 主要作为存储元信息的管理者
2. Secondnamenode 是作为namenode的秘书
3. Datanode 作为工作者

---
### namenode 启动过程
1. 第一次启动的时候先格式化文件系统,为了生成镜像文件
2. 然后启动namenode读取镜像文件,把文件内容加载到内存里,然后等待datanode注册
3. 启动datenode,datanode就向namenode注册并发送自己的块信息
   
第二次再启动namenode
1. 读取镜像文件
2. 然后合成新的镜像文件

### datenode使用过高怎么配置
配置里面有个优先写空间多的磁盘,本来默认是轮询的

如果是问堆内存使用率过高
1. 将HDFS备份数降低
2. 删除无用HDFS数据和Hbase表格
3. 设置kafka的日志时间
4. 删除本机无用文件
5. 清理Trash回收站
6. Balancer重新平衡

---

### Hdfs写数据流程
1.	客户端带着文件路径通过RPC去找namenode问能不能上传,然后namenode就检查该文件是否存在以及客户端是否有权限,返回可不可以
2.	客户端就会请求上传第一个Block块
3.	Namenode就会根据**副本放置策略**去进行节点分配回给客户端datanode地址
4.	客户端就去和第一个节点去打通**pipleline**管道,第一个节点再去和第二个,第二个在和第三个节点去建立管道,然后逐级返回信息给客户端
5.	客户端收到消息后就开始传输第一个block块,将内容读取到一个4kb的buf中,读满一个buf就往外flush出去,存到一个512b的chunk以及4b的checksum校验内容,接着将许多的chuck存满一个64kb的pactet包,然后将包放到dataqueue传输,传输出去后就放到ackqueue用来等待接收pipleline返回的应答
6.	Datanode收到packet后先将这份文件序列化到磁盘上,然后再继续传输到下一个datanode上,直到传完,发送ack应答消息
7.	客户端收到应答消息觉得没什么问题就移除应答队列的文件,一直到发完所有的东西就关掉管道.

---

### HDFS机架感知(选择哪个datanode节点)
- 第一个 block 副本放在 Client 所在的服务器，如果 client 不在集群服务器中，则这第一个 DataNode 会随机选择。
- 第二个副本放置在与第一个节点不同的机架中的节点中，保证机架间的高可用。
- 第三个有不同机房则跨机房随机放置在某个节点上；只有一个机房则和第二副本在同一个机架，随机放在不同的节点中。
更多的副本，则继续随机放置，需要注意的是一个节点最多放置一个副本。


### HDFS文件数据倾斜的解决
配置一些datanode的balance参数 
开启数据均衡 hdfs balancer 然后重启namenode和datanode 然后


### HDFS支持多个用户同时写吗
在 HDFS 的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前 HDFS 还不支持多个 用户对 同一文件的写操作，以及在文件任意位置进行修改。

### 那在写数据过程中失败了会发生什么
第一种是datanode在传输前出问题了,那就把这个节点从管道里踢出,重新在找一个
第二种是传输过程中出问题,先尝试唤醒那台宕机的,如果叫不起来就申请新的节点,重新打通管道,然后更新generation stamp 版本号
,当那台死去的机器复活了,Namenode就知道他版本号不对劲,就让他删掉那个损坏的block块




### Hdfs读数据
1.	客户端会带着读取路径也是RPC连接方式去问namenode ,喂喂喂我要找的东西在哪呀,然后namenode就检查有没有这个文件,有就跟返回信息回去,告诉他那些block块在哪里
2.	然后按照集群拓扑能得出datanode与客户端的距离,客户端就会找最近的下载下来
3. 底层上本质是调用DataInputStream的read方法,并行读取block信息
4. 读取完一个block都会进行checksum验证,如果出现错误就通知namenode,然后再从另一个节点读取 
4.	结束后就关闭输入流


---

### SecondaryNamenode 的工作机制是什么？(namenode元数据管理)
首先是*namenode*自己有一个日志文件和镜像文件,SNN默认会每一个小时或者有一百万次日志操作就会通知一次NN去滚动日志信息,然后将NN的日志文件和镜像文件都给下载到自己的本机上,将镜像文件反序列化到内存后重演一次日志文件再序列化进自己磁盘生成新的镜像文件,再把新的镜像文件返回给NN 

---

### Maprudece的shuffle阶段描述一下
1.	首先shuffle阶段是map方法之后,reduce方法之前的这段过程.
2.	map方法之后数据首先进入到partition分区,这个分区默认是通过计算key的hash值后对ReducekTask的数量取模获得,然后将任务不断的将键值对输出到内存的默认大小为100M环形缓冲区,达到了80％之后对数据进行溢写, 溢写前会对数据进行按照对key的索引进行字典排序的快排,然后将大量的溢写出来的文件进行归并排序,这过程叫merge,最后将文件分区存储到磁盘上,等待reduce端复制.
3.	当reduce获知到map生成的文件位置后,建立HTTP连接,去复制文件时先一样将数据写入到环形缓冲区,到达阈值时将数据批量溢写到磁盘,然后进行归并排序,归并排序完对文件进行分组. 
---

### 简述MR的工作流程
1.	待处理的文本通过submit()方法，获取待处理的数据信息，然后根据切片机制，生成切片文件。把切片方法文件和一些配置文件提交在资源路径。 
2.	Yarn就会根据文件的切片信息去计算将要启动的maptask数量,接着去启动它,maptask就去调用inputformat方法去HDFS上面读取文件,Inputformat方法又再去调用RecordRead方法,将数据以行首字母的偏移量为key,一行数据为value传给自己写的mapper方法
3.	进行mapper方法逻辑处理之后就将数据传到分区方法中,对数据进行一个分区标注后送到环形缓冲区里,环形缓冲区的作用是批量收集mapper结果,减少磁盘IO.
4.	在溢写之前会做快速排序,然后溢写出来产生的文件也会进行归并排序,然后将文件存储到磁盘上,再启动一定量的redeceTask
5.	reduceTask就会复制那些文件到自己的缓冲区中去溢写,等数据拉取完毕会进行一次归并排序,归并完通过GroupingComparator方法对文件进行分组,然后将相同key值的数据调用自己写的Reduce方法,一次读取一组
6.	最后调用Outputformat方法里的RecordWrite的方法将数据以KV的形式写出到HDFS上



### 溢写阶段详情
溢写阶段详情：
1. 利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，
然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有
序。
2. 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。
3. 将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件
中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件
output/spillN.out.index中。


### MR的为什么要归并排序
为了减轻Reduce端的内存消耗

---

### yarn的基本组成
1. ResouceManager（简称RM）：
资源管理器负责整个集群资源的调度:调度器（Scheduler）和ApplicationsMaster（简称ASM）。
调度器会根据特定调度器实现调度算法，结合作业所在的队列资源容量，将资源按调度算法分配给每个任务。分配的资源将用容器（container）形式提供，容器是一个相对封闭独立的环境，已经将CPU、内存及任务运行所需环境条件封装在一起。通过容器可以很好地限定每个任务使用的资源量。yarn调度器目前在生产环境中被用得较多的有两种：容量调度器(默认)（Capacity Scheduler）和公平调度器（Fair Scheduler）,FIFO调度器不经常用。

2. ApplicationMaster（简称AM）：
每个提交到集群的作业（job）都会有一个与之对应的AM来管理。它负责进行数据切分，并为当前应用程序向RM去申请资源，当申请到资源时会和NodeManager通信，启动容器并运行相应的任务。此外，AM还负责监控任务（task）的状态和执行的进度。

3. NodeManage（简称NM）：
NodeManager负责管理集群中单个节点的资源和任务，每个节点对应一个NodeManager，NodeManager负责接收ApplicationMaster的请求启动容器，监控容器的运行状态，并监控当前节点状态及当前节点的资源使用情况和容器的运行情况，并定时汇报给ResourceManager。

---
### yarn的默认调度流程,调度器分类,以及他们的作用(调度策略)
1.	FIFO先进先出调度器 同一个队列的先提交的就先执行,后面的排队等待
2.	Capacity 容量调度器 多个任务队列同时进行,但每个队列也是先进先出(hadoop默认)
3.	公平调度器 可以占用别的队列的资源 然后等别人需要了在还回去(CDH默认) 

---


### yarn的资源调度工作流程
1.	客户端向YARN中提交应用程序
2.	作业提交后，ResouceManager根据从NodeManage收集的资源信息，在有足够资源的节点分配容器，并与对应的NodeManage进行通信，要求它在该容器中启动ApplicationMaster。
3.	ApplicationMaster首先向ResourceManager注册
4.	ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。
5.	一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。
6.	NodeManager为任务设置好运行环境后,启动Task。
7.	各个Task向ApplicationMaster汇报自己的状态和进度，并于ApplicationMaster保持心跳,以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。
8.	应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。

### yarn的设计思想
YARN的基本思想是将资源管理和调度及监控功能从MapReduce分离出来，用独立的后台进程实现。这个想法需要有一个全局的资源管理器（ResourceManager），每个 应用还要有一个应用主管（ApplicationMaster）。应用可以是一个单独MapReduce作业，或者是一个作业的有向无环图（DAG）



---

### Hadoop解决数据倾斜的方法
1.	提前在map端comblne,相当于在map端就reduce了,但如果是导致数据倾斜的key分布在大量的mapper上,这方法就没用了
2.	这时候就需要局部聚合加全局聚合,第一次map阶段给那些key加随机前缀,分配到不同的reducer ,然后再通过第二次map去掉前缀,进行全局聚合
3.	或者增加reducer 提高并行度
4.	又或者可以自定义分区


SQL：有一张表记录了每天的下单用户信息，日期|用户ID|事件类型(下单|付款|退款)|事件时间。
计算用户的近30天(第2天-第30天)的复购率(付款成功且未退款的) 你说一下思路

---

### 为什么HDFS中的块(block)不能设置太大，也不能设置太小
1.	如果设置过大,就会导致传输数据的时间远大于的寻址的时间,导致程序处理这块文件变得很慢,map阶段也会很慢,也会使得网络传输时间变成,且失败重试的成本就会很高
2.	如果设置过小,就会造成存放大量小文件的现象,而且寻址时间就会变多.
3.	设置128M的原因:其实和磁盘的传输速度有关 具体的不记得了


---

### Hadoop中combiner和partition作用
1.	partition的作用就是把数据归类,就会对每个reduce建立分区进行计算,加快计算效果
2.	combiner相当于一个优化方案,优化map和reduce的数据传输量,差不多等于在map端也进行了一次reduce
3.	combiner只应该用于reduce输入的keyvalue和输出的keyvalue类型一样的而且计算逻辑上combiner不能影响计算结果,像求平均值这种就会影响


### MapReduce中的Combiner
1. combiner是MR程序中Mapper和Reducer之外的一种组件
2. combiner组件的父类就是Reducer
3. combiner和reducer的区别在于运行的位置：
         Combiner是在每一个maptask所在的节点运行
         Reducer是接收全局所有Mapper的输出结果；
4. combiner的意义就是对每一个maptask的输出进行局部汇总，就可以减小网络传输量
5. combiner能够应用的前提是不能影响最终的业务逻辑
而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来

具体实现步骤：
   1. 自定义一个combiner继承Reducer，重写reduce方法
   2. 在job中设置：  job.setCombinerClass(CustomCombiner.class)



---

### Datanode怎么保证数据的可靠性的
和传输时候的checksum有关


### HDFS上传文件损坏了,不通过副本机制,该怎么处理
1. 使用hdfs -fsck命令检查数据块哪些丢失
2. 再用recoverLease 去修复尝试多次修复文件
或者是使用纠删码,但这个我只知道hadoop3.0的新特性,但具体不知道这是什么

### datanode 负载均衡问题
HDFS 自带得Balancer参数 有个调整平衡的 还有个占用带宽的就能搞定
还有hdfs-site.xml有参数可以设置

### HDFS压缩有什么
gzip  snappy izo bzip2  