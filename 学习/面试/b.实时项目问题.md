### 项目介绍
我们战略之眼这个项目严格意义上的来说是是一个事件驱动型的应用，不属于通常的数据统计开发，是一个业务导向的系统，主要的功能为我们的业务人员在web端经过画像标签特征圈选好所指定的人群，然后通过采集在线用户的行为，然后对这些人群进行规则匹配,然后触发一些营销手段。
举个例子，比如对消费能力等级在3以上，并且在最近5天内有过收藏重庆民宿十次且分享次数达到三次，当她再次搜索重庆民宿的时候，就立即给她推送一个299-20的活动券通知，从而促成她的下单消费；

亮点也就是我们做到了任意规则的计算模型+同一种模型参数可以任意变相结合.

1. 架构想象为一张图，上半部分为数据静态接入模块，而左边为从kakfa实时采集用户行为日志后预处理得到的数据，存入到Doris中，右边为离线数仓组所提供的用户画像，以ElasticSearch存储。
2. 中间部分为规则注入管理模块
   1. 那运营人员会选择人群以及制定规则嘛，发送过来的json串，我们是以enjoy模板代码来动态解析更改，然后以groovy脚本去注入，人群圈选就会将人guid形成bitmap，然后一并存入到Mysql元数据库里，而后续的增添规则或者维护工作，我们只需要去修改Mysql元数据库就能实现。
   2. 到这里，Doris的作用是什么呢,Doris的作用就是里面存放着一些历史数据做为规则条件的初始值，也就是Doris的数据放入到redis集群中作为Flink的状态他就变成了我下面要说的运算机的初始值，就比如我今天发布规则，而我圈选的日期是横跨了历史和未来的，那么所用到的历史数据就从Doris来,然后再进行计算模块的累计和判断。
3. 下面部分为规则计算引擎模块，也就是从Mysql的规则模板表用Flink-CDC监控然后广播出来，然后和Flink接kafka的实时数据进行连接,将这条行为信息进到每个规则运算机去判断是否符合条件，符合条件的就将运算结果输出出去，而不符合就让Redis状态数据进行变化，比如说用户今日浏览上海民宿1次,没能达到规则要求,那么redis的状态就给他+1.一直到该用户规则满足后再触发。
项目的大体就是这样。



### 为什么不用Flink CEP
一开始的时候，我感觉这个需求跟flink cep的适应场景很匹配，可是后来跟需求那边详细沟通了一下后，发现他们有一个非常关键的需求点就是： 要求能够在job不停的情况下，在线动态地改变运营规则，比如可以发布新规则，下线已运行规则，或者重新上线已经停用的规则，修改已运行中的规则的参数等等，而且都要求能够实时生效；

所以如果用flink cep的话，一旦把规则写好，就定死了，如果需要动态增加或者修改规则，必须新开发job，或者修改原来的代码，然后把原job下线再上线新的job，无法满足营销人员的快速灵活营销需求；

所以我们是自己设计了一套机制和逻辑，来实现这个系统；用到的技术栈主要就是flink的cdc、广播、processFunction，另外还有用于注入动态逻辑的groovy，还有就是用于查询行为明细的doris库和查询画像属性的ES；


### 为什么要用redis
1. redis对外开放，需要对运算结果或初始值等进行人为干预的话，很方便
2. redis中有丰富的数据结构，因为中间聚合结果的数据结构会有差别,我们要拿这个状态与未来在线运算的相结合嘛,这也会为我们的滚动增量运算逻辑设计带来潜在的帮助
3. redis本身的读写速度与flink内部的state不相上下，可以满足我们的高并发低延迟读写要求,也仅仅只是丢失一丢丢checkpoint回滚的特性,但我们不用去计较这种错误,最多造成的也只是他获得了一个我不该发的优惠券而已,没必要去为了精度丢失掉时效性就或者性能.


### 关于redis中的数据容量的问题
redis中，一个hash结构中的kv条数最多就是整数最大值嘛，而我们的一个规则的一个条件状态记录hash中
，也就30万人左右（30万个元素），很轻松；

站在整体来看，比如上线了50个规则，每个规则平均4个条件，那就有200个条件，那就是200个hash，也是很轻松的；

而且，如果redis负载能力不够的时候，可以部署redis集群的；比如5台机器；那么200个hash就可以分布到5台机器上，相当于每台机器只要负载40个hash结构即可！很轻松！


### Groovy是什么
groovy是一种动态脚本语言,在动态编译加载的时候比java更加的简单,反射后得到对象就能直接调用计算方法得到数据,在我们这个项目就是用来做不同规则模型的导入

### redis 标签存储格式
规则序号：条件序号 --> （guid,次数）


### 你们的qps多大?
您是说app嘛，我们公司日活120万左右,峰值时候同时在线人数4万,qps为1.2w左右。
flink配置了12台服务器

### 日活 月活 数据量 条数 
70-80万   300万  100G数据   


### 为什么要用Kafka而不用MQ这种
1. 在消息队列这个圈子里Kafka是当下最流行的，认可度也是最高的，社区支持力度最高的
2. 高度成熟的技术和庞大的生态可以兼容当下所有的主流计算引擎，丰富成熟的API也对开发者友好


### 规则sql模板存在mysql的字段
1. 规则模型id
2. 条件类型 大于小于
3. sql模板 text
4. 作者
5. 审核人
6. 创建时间
7. 修改时间

### 规则引擎资源字段
1. 模板id
2. 规则id
3. bitmap
4. json
5. groovy代码
6. 状态上下线 

### 规则模型计算模板
规则计算机 


### 从kafka到flink规则计算引擎，如果有5个分区，怎么保证数据从5个分区取得的数据是有序的
因为我们上游数据采集系统在写数据到kafka的时候,用了自定义分区器,就把相同用户的数据写到了相同的分区,这样下游消费的时候,同一个用户的数据就不会乱序了


### 你们数据会有数据倾斜问题么
几乎不会有,因为我们keyby是以guid来分区的,但为了避免有搞鬼的搁这用什么奇怪的东西猛点然后全是他的数据,所以我们还是在此基础上用了两阶段聚合的方法冲散key值.


### 难点之一 动态Keyby
我们的规则需求中，一开始，运营人员所指定的规则，在计算时基本上都是按照用户为单位进行计算，所以，我们的整个处理逻辑中，是把数据先keyby（用户id）后，再进行核心逻辑的计算的；  但是后来，需求那边偶尔也会提一些“风控类”的规则，比如“同一个ip地址在指定时间跨度内，出现过N次登录密码错误”等规则，那这些规则对匹配计算的逻辑上，需要按照ip字段来进行keyby

所以，如果代码中把keyby写死，则无法适应上面所说的这些需求了；

为此，我们设计了一个动态keyby的方案；
简单来说，就是，首先要在注入的规则参数中，要按照我们的规范指明：这种规则的匹配计算需要按照数据中的哪个字段进行keyby
然后，在我们的数据处理逻辑中，我们会先把数据按照各个规则中的keyby要求，提取出所需要的所有“keyby”类型，比如（有按deviceid的，还有按ip的），那么，我们就利用反射手段，从数据中提取出指定的这两个字段的值，然后把一条数据“复制”成两条，分别带上deviceid的值和ip的值，并命名为“keyByField”字段，后续在keyBy时，就写 keyBy(bean->bean.keyByField)就ok了.


### 你们用的什么什么时间语义？万一有数据延迟到达怎么办？
我们的这个项目强调的是**时效性**，而不是数据统计的**完备性**,
如果用时间事件语义，那么当数据乱序迟到时，可能会导致我们整个时间推进延后！不符合我们的时效性要求！
我们在这个场景中，采用的是process time时间语义，而且，我们的程序并不是用window算子等来实现的；而是处理逻辑完全由我们自主控制的，我们可以在规则运算机中根据需求，判断当前的系统时间以及到达的事件的行为时间，是否满足我们的要求做出不同的处理！

### 数据重复消费问题你们怎么处理 
重复消费的这个问题,那就证明了在收集过程中产生了故障,那故障的问题也并不是每次都产生了重复对吧,还会产生其他的问题,那就是排查bug的问题了,如果您说的是精准一次的问题,那我们并不会去处理这个,因为这个重复并不是一直在产生的,产生的后果也就是发了个不该发的优惠券而已,我们本身就是希望他消费,所以没那么大的关系.



###  发布规则之前会查询动态画像条件历史值并发布到状态，然后规则再上线运行，这中间有一点时间差，那么，是不是会产生计算的遗漏？
哇这个问题好牛。
这个问题确实存在！
只是，在咱们这个智能运营的场景中，能够容忍一定程度的不精确；
另外，这种漏算数据的概率其实也很小，第一，时间差很小；通常在10s以内；第二，在这段时间差内，用户的行为数据中也不一定就包含规则条件中所需要的事件；

当然可以，我们在下一个版本的迭代计划中，已经设计了相应的方案，而且在测试环境中已经经过了测试；
方案是这样的；
1. 我们添加了一个功能，也就是能够一次性接收一批用户行为事件进行处理,用ListState去获取,并且这个TTL设置为30s,也就是说假设我10点55秒的时候这规则上线了,那我状态里的数据就是25-55秒的,我这一批都给规则运算机去,然后它自身就会判断应该要从什么时候开始截断,也就是说doris查询历史值的一瞬间是10点50秒的话,那sql语句查询窗口的end也就替换成50秒,它从元数据库发送到redis的时候,我们也就顺便查一下这个截止时间点,然后再去取到状态里的数据,这批25-55秒的数据,我们只要50-55秒的.
2. 具体也就是在flink引擎中，添加一个机制
来一条用户行为事件,我们就遍历每一个运算机,因为运算机会有一直在跑的嘛,会有多条规则一直在上线着,那就判断这个运算机是不是第一次运行的,如果是的话,就把这个存储了30的状态丢给运算机去批处理,批处理完之后,再把新来的这条用户行为事件去让他正常处理,如果不是第一次运行的状态机,是之前一直在跑的,那就让他正常处理.


### 这个项目怎么做成实时画像
ES改成Hbase或者Clickhouse,但要要求配置足够好,这样就可以动态的去查,就不需要像我这个项目的做人群圈选了.


### 这个项目我们为了让用户数据有序用了kafka自定义分区,那自定义分区对什么进行分,如果用guid的话那预处理过程的guid怎么搞定..
我们这个项目不做guid了,因为我们在离线数仓那边发现,需要去更改guid的人很少很少,所以我们决定用设备id来决定分区(用guid还会使得整套流程变慢,会很麻烦),这样也能保证有序性

### kafka自定义分区之后和flink对应的并行度一致,那flink还用不用keyby
如果仅仅是要去决定是否有序的情况下,我们是可以不需要对他进行keyby的,但我们必须要keyby的原因是,我们的状态是keyedstate,存入state的数据是一定要一个人对应一个数的,所以是必须要keyby来发配的

### 这个项目整套流程会不会产生数据倾斜的问题
这项目是会有数据倾斜的问题的,我们实测会发现,晚上8点到8点半的时候,会有一部分的Task涌入量会比其他的大很多,但我们的配置就是用来解决这个的,所以配置设置了比较大,超出峰值还要多一点,而且这个量也只是一段时间内的,有可能是这个task在这个半小时内会大量数据,但下一个时刻他数据就会变小,所以我们可以解决这个问题