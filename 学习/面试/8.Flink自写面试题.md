
### Flink的四大基石
1. Checkpoint重启策略  Checkpoint 本质就是对状态的备份, 持久化到文件系统

2. 状态 用来保存中间计算结果或者缓存数据
旧版 
MemoryStateBackend  基于内存存储
FsStateBackend  基于文件存储
RocksDBStateBackend  基于RocksDB数据库存储 
新版
状态后端有两种存储 (底层的具体实现)
HashMapStateBackend  存什么就是什么类型的 
RocksDbStateBackend  以序列化的形式存储在rocksDB数据库中 
主要在于效率问题 hashMapStateBackend快  因为放在内存中 多出来的溢出在磁盘
弊端为内存空间有限,所以在磁盘的会慢 所以数据量不算大的时候用这个

状态类型有
托管状态和原生状态
然后分成 KeyedState 和 OperatorState 

1. Time 时间语义 

2. Window 将无限的数据流划分成多个有限数据流的手段 希望将数据攒起来做微批次的处理 
事件时间的度量标志为水位线


### window窗口有哪些
滚动 滑动 会话

### watermark的意义
1. 标志着Flink任务的事件时间进度，就能推动event时间语义窗口的触发和计算
2. 解决eventt时间窗口的乱序问题,就是设置延迟时间,或者是窗口内排序什么的


### watermark的生成策略有哪些
1. 使用固定延时策略生成水位线，调用WatermarkStrategy中的静态方法forBoundedOutOfOrderness。
2. 使用单调递增的方式生成水位线，调用WatermarkStrategy中的静态方法forMonotonousTimestamps
3. 使用event时间的获取
4. 处理空闲数据源

### Flink时间语义
1. 处理时间（Process Time）系统时间
2. 事件时间（Event Time）数据源提供的时间
3. 注入时间（Ingestion Time）数据进入flink的时间
Flink在1.12版本后默认使用Event Time

--- 

### 如何保证数据不丢失
Flink自身不能完全保证数据不会丢失，最多只能通过设置较大的延迟时间来让乱序的事件到达。


### Flink保证Exactly Once语义
举例kafka保证Exactly Once
1. source算子 把kafka消费者的消费位移记录在算子状态中
2. 中间算子 也就是flink的两阶段提交嘛
3. sink算子
   1. 采用幂等写入方式
   2. 采用两阶段提交事务写入方式
   3. 采用预写日志2PC提交方式

---

### flink两阶段提交(和两阶段提交事务无关)
两阶段提交是保证source端精准一次嘛
具体的是flink在checkpoint的时候会有一个预提交的动作,然后算子这一轮保存完之后要去和jobmanager说:领导!我们checkpoint成功啦,然后jobmanager在给他们所有算子发okok这个正式的提交,这才表示这次checkpoint完成.但是领导的提交命令是异步的,可以让其他人做别的事.

如果领导的提交命令出问题了导致checkpoint重复,那么就可以在预提交阶段的时候把事务信息保存在状态里,失败了的话就从状态里面恢复事务的信息,然后在checkpoint结束方法完成对事务的提交.



### Flink容错机制
重启策略 本质就是对状态的备份, 持久化到文件系统

一个为checkpoint，一个为savepoint，checkpoint默认为自动保存到状态后端。
而且savepoint的数一般要比checkpoint的更新一些


### flink checkpoint的原理 以及怎么实现的
Flink分布式快照算法,异步 barrier 快照

### 检查点分界线 barrrier  分布式快照算法
形象化的来说，jobmanager会给每个任务做完的时候插上木板拦截下来让他分身拍照，另一份数据会继续往下流。
通过插入序列号单调递增的barrier，
把无界数据流划分成逻辑上的数据段，
并通过段落标记来为这段数据的处理加持事务的特性，
就有了每一段数据流要么全部一次处理成功，要么回滚一切不完整的任务。


### checkpoint原理描述
jobmanager内部有一个checkpoint协调器,checkpoint触发之后就向TaskManager中的subtask发送消息,让subtask中自己持有的状态保存到HDFS中或者别的地方
然后所有TaskManager返回通知给jobmanager,此次checkpoint才算已经完成,如果中途有个出现异常,jobmanager会将此次checkpoint全部取消



### 有没有遇到过checkpoint失败
1. 我们一般回去web ui界面的任务日志去看看错误详情
2. 之前遇到过两个 一个是因为数据倾斜所导致的，我们就将key加随机数打散就ok了
3. 第二个是 我们设置了重启次数嘛，然后还是一直报错，我们就给checkpoint增加并行度增加超时时间，一点用都没有
   1. 然后我们就炸开算子链一个一个的排查
   2. 后来发现我们需要去mysql查询这个数据是否满足条件，由于表很大的缘故里面的sql又写的很差导致长时间没办法返回数据
   3. 就修改sql就好了

--- 

### Flink八大分区策略
1. GlobalPartitioner 该分区策略将上游的分发到下游算子的第一个实例中
2. forwordPartitioner 该分区策略用于在同一个operatorChain(算子链)中进行转发,上下游并行度需要一样
3. shuffle partitionner 该分区策略会将元素进行随机分区
4. rebalance partitioner 该分区策略以轮询的方式去为元素分区
5. rescale partitionner 该分区策略会根据上下游的Task数量进行分区,根据节点轮询
6. broadcast partitioner 该分区策略会广播给所有分区,每个节点各有一份复制在自己手里
7. keyGroupSteam Partitioner 该分区策略根据keyGroup的hash进行分区
8. 用户自定义分区 需要用户实现partition接口来实现自己的逻辑

----


### Flink提交模式
session模式 多个job共享同一个集群，job退出了集群不会退出
（大量小job的时候比较适合，因为不用频繁向yarn注册应用）
提交job先启集群 再提job

perjob模式 每个job独享一个集群 job退出集群也退出 main方法在服务端上运行
（大job运行时长很长的时候比较适合）
(我们的任务并不是很多,所以用这个提交方式的资源隔离机制更好，因为当Job发生错误时只会让自己集群的taskmanager挂掉，而且他还能把负载分散到多个jobmanager上)

application模式 
每个Job独享一个集群
job退出集群也退出 main方法在集群上运行
提交job的命令即是集群的又是job的（底层会使得main在集群上启动）



### Flink提交job的流程(Flink工作原理)(任务提交)
1. 客户端向 ResourceManager 提交Job,然后yarn里面的 ResouceManager 接到请求后,先分配好容器,然后通知 NodeManager 启动 ApplicationMaster。
2. ApplicationMaster启动JobManager， 然后 JobManager 会分析当前的job的作业图,然后把它转化成执行图，然后就知道当前需要什么的具体资源。
3. 然后,JobManager 会向 ResourceManager 向yarn申请资源，ResouceManager收到指令之后,就去申请容器的资源， 然后叫ApplictaionMaster 启动一系列的 TaskManager。
4. TaskManager 启动后，会向 JobManager 发送心跳以及注册slot槽。JobManager 就直到有活人了!快来工作!就去给 TaskManager 分配任务。


### perjob
区别只在于 JobManager 的启动方式，以及省去了分发器。当第 2 步作业提交给JobMaster，之后的流程就与会话模式完全一样了。

应用（Application）模式
应用模式与单作业模式的提交流程非常相似，只是初始提交给 YARN里面的ResouceManager 的不再是具体的job，而是整个应用。一个应用中可能包含了多个作业，这些作业都将在 Flink 集群中启动各自对应的 JobMaster。



---

### Flink CEP
就是把复杂的事件一系列串联起来成一个事件流去监控.具体的不太了解了.

### 你们公司Flink用什么监控的 
普罗米修斯+Grafana顾罗方娜  检测Flink的度量Mertic 
有时候也会去flinkweb端页面去看


### 1 小时的滚动窗口,一小时处理一次的压力比较大,想让他5分钟处理一次.怎么办?
自定义触发器，重写4个方法



### window 后面跟 aggregate 和 process 的两个窗口计算的区别是什么？
1. aggregate： 是增量聚合， 来一条数据计算完了存储在累加器中，
不需要等到窗口触发时计算；
2. process： 全量函数， 缓存全部窗口内的数据， 满足窗口触发条件
再触发计算， 同时还提供定时触发， 窗口信息等上下文信息；
3. 应用场景： aggregate 一个一个处理的聚合结果向后传递一般来
说都是有信息损失的， 而 process 就可以可以更加定制化的处理。



### Flink旁路缓存 
也就是所有请求优先访问Redis缓存，若缓存命中，直接获得数据返回给请求者。如果未命中则查询Hbase数据库，获取结果后，将其返回并写入缓存以备后续请求使用。


### flink 常用算子
map reduce flatmap keyby window broadcast union join connect 

---

### flink内存管理
分为堆内内存和堆外内存 
堆内内存
1. 网络缓冲区：这个是在TaskManager启动的时候分配的，是一组用在缓存网络数据的内存
2. 内存池：用在运行时的算法（Sort/Join/Shuffle等），这写算子启动的时候就会分配这个内存池。默认情况下，占堆内存的70%的大小。
3. 用户使用内存: 这部分的内存是留给用户自定义代码使用的。

Flink堆外内存
堆外内存,好像是用来执行一些IO操作

---
### Flink的广播实现原理
有点像hive的mapjoin,它是将一个吞吐比较小的一个流的数据做成mapstate广播到下游每一个Taskmanager内存里,然后每个task就都可以去内存里去拿.



### Flink 需要划分Task的情况
Flink划分Task主要有四种情况：
1. 类似keyBy，broadcast，rebalance等算子产生shuffer
2. Parallelism（并行度）变化
3. new chain，即在算子上执行startNewChain()后，该算子与前面执行的算子分开。
4. disableChaining，在算子上执行disableChaining()，即算子的开始到结束，单独生成一个task。使用场景，比如该算子逻辑复杂，让算子独自使用一个task内的SubTask。


### Flink 并行度 slot taskmanager的关系
taskmanager 包含多个槽  所以并行度最好为 slot*taskmanager  
关系就是 并行度如果设置成一个 那九个槽只能用一个槽 其余八个不动      


### flink中的反压机制
**含义**:下游算子处理数据的速度跟不上上游算子发送数据的速度,这时需要对上游算子进行限速

**产生的问题**:
1.数据阻塞,barrier到达算子的时间变晚,导致整个checkpoint周期变长了,数据一直堵在上面;
2.或者是有两条输入流的算子,因为checkpoint会导致先收到barrier的那条流需要等待,然后对后序到来的数据就要缓存在state中,导致state变大

**反压如何定位**:1.通过flink web ui自带的反压监控面板(ok,low,high);2.通过Task Metrics 分析,主要观察buffer占用(subtask发送端buffer占用率高,说明被下游反压了,接收端buffer占用很高,说明将反压传导至上游)

**如何解决反压问题**:
导致反压出现的问题可能包括:数据倾斜,代码执行效率,taskmanager的内存以及GC问题等,最常见的还是数据倾斜问题


### flink数据倾斜的sql


### Flink 数据倾斜(热点问题)
1. 首先我们会通过flink的webUI查看相同task的subtask的数据量的情况,看是否出现某些subtask中的数据量明显大于其他subtask的数量;

2. 我们会分为keyby前和keyby后嘛
   1. 在keyby前,我们设置好最大并行度,加上我们会用shuffle,rebalance,rescale等这种改变分区的算子让他合理的分布在每个区上


   2. 在keyby后我们会用两阶段聚合的办法,将加上随机前缀冲散key值然后再聚合时去掉后缀.
   但如果妹有窗口的话,我们就用flatmap让他变成批次去避免重复计算的问题


```sql 
SELECT id,sum(sum_level) sumLevel
FROM
(
	SELECT nk,sum(`level`) sum_level,id FROM 
	(
	select id,`level`,concat(id,'-',ROUND(10*RAND(),0)) nk
	FROM
	test
   where 1=1
   and id is not null ;
	) t1
	GROUP BY nk
) t2
GROUP BY id
```



### Flink 种Connect 和 join  union  cogroup broadcast的区别
1. join只能两个dataStream或者两个dataSet一起join,操作DataStream的时候只能用在window中,和sql的innerjoin是一个意思,只能输出两个窗口或者两个集合的时间内能匹配上的,没匹配上就不输出.
2. cogroup 基本和join差不多,但是它在一个流或者数据集中没有找到与另一个匹配的数据它还是会输出。
3. connect它只适合操作DataStream,而且只能两个流一起connect,他们可以共享同一个状态.
4. union流和数据集都可以,而且可以多个流一起合并,而且不会去重,但是一定要是一样类型的流或者数据集.


### Flink双流Join
1. window join 也就是将无界数据流变成有界数据流，窗口时间到就触发，触发的内容取决于是inner join 还是outer join(outer join会有没join上的数据)

2. interval join 两条流的数据从无界变有界，也就是说A流可以joinB流的时间范围，而window join是两个流的时间join，但如果是outerjoin，那么需要数据需要等待另一条的区间结束了才能得到。一般用于去评估两条流的延迟时间。

3. regular join 无界数据流尝试去关联 因为底层是回撤流，关联不上的先下发，等到能关联上就关联，所以完全保障了数据的时效性。（只能sql实现）
   1. 需要的是sink组件要支持回撤，比如mysql，如果传入的是kafka的话，消费Kafka的也要支持回撤。

常见问题就是状态会变大
我们解决的方法一般都是让flink任务轻量化，安排一个redis作为状态后端，相同key的数据维护在一个hashmap里


### Flink维表join问题
1. 小的维表我们就广播这个表
2. 又或者是用open方法存在内存里，这种就适用维表更新不频繁的
3. 或者是将维度数据导入到redis中，通过异步IO的方式去进行查询。
4. Temporal join 也就是将维度数据映射为一个虚拟表，用主流去join这个虚拟表 
   1. (比如我们公司有一个汇率订单表，每个订单都有不同货币的价格。为了将此表正确地规范化为单一货币，每个订单都需要与下订单时的适当货币兑换率相结合。)


### 数据延迟或者乱序的问题
1. watermark上设置延迟时间 第一层防护
2. 用那个允许迟到的算子allowedlateness,做更后一步的推迟等待 第二层防护
3. 实在不行就设置测流接迟到更久的数据做处理方式然后跟主流后续累加。 第三层防护




### Flink CDC 和 Debezium(DB字母)  canal(啃尿)区别 （这些是cdc替代品）
FlinkCDC底层就封装了debezium 
debezium 单机  canal也是单机而且不支持全量