
### 分析过的指标
日活 月活 留存 新增
流式 转化 三天点赞\收藏 热度排行 优惠券人数 点赞

留转G复活指标:活跃 GMV 转化率 留存率

坏房数(因装修不能出售的)
Occ客房入住率 
ADR已售客房平均房价 
RevPAR 可供出租客房的收入 
Revenue月收入
CTP 营业利润贡献
GSS 宾客满意程度指数
MPI 市场渗透指数 
订单量=曝光量X转化率-取消量
1. 民宿房东维度:
   1. 销量收益，合作度，活动参与度，违规评分，三十天内无违规 无拒单 评分 回复率 
2. 用户维度
   1. 搜索场景(同城搜索，异地搜索)，评价率，累积访问次数，性别，年龄，提前预定天数，入住率
   2. 活跃指标: DAU活跃用户数 活跃率 在线时长 启动次数 PV UV

### 做的宽表
访客页面历史记录表
优惠券粒度订单记录表
各省份粒度订单记录表
学生特惠粒度订单记录表
网红民宿信息记录表等等

比如房东房源的订单记录表,我们会往里面塞一大堆维度退下来的,比如评论,比如评分,点赞 70个字段
装修风格,地段,入住率

Occ客房入住率 
ADR已售客房平均房价 
RevPAR 可供出租客房的收入 
Revenue月收入
CTP 营业利润贡献
GSS 宾客满意程度指数
MPI 市场渗透指数 

用户的
签到量
浏览量
点击量
CTR 点击量/浏览量


### 宽表的设计原则,或者说你们是怎么设计宽表的
我们开发宽表就是为了避免重复迭代,得需要了解一下业务全流程,知道需要扩展哪些维度
有一部分的属性描述信息,另一部分就是度量值
设计的话我们就会去思考这个宽表到底会有多宽,能不能少一些字段,然后不要让这个宽表跨主题域
然后分清楚主次关系,做个热词和冷词汇的分离.

### 数据量最多的表
用户行为宽表
多的话能有4-6G左右



### 说说你最擅长的业务模板 

### 关于主题
主题:根据统计需求,将具有相似统计特征的内容放在一个分组上,方便后续的计算维护管理等
1. **流量主题**  
   1. 页面分析报表
   2. 会话分析报表
   3. 用户分析报表
   核心的度量比如有pv,uv,会话数,跳出会话数,访问时长等等
   DAU活跃用户数 活跃率 在线时长 启动次数 
   核心的维度有,时间维度,新老访客,地域,终端维度,入口页,退出页等  

   高阶函数 with cube | grouping set |  with  
   类似多维分析主题 广告分析 交互时间分析  运营活动分析

2. **用户活跃主题** 
   1. 会话次数
   2. 访问时长
   3. 连续访问天数
   4. 性别比例
   5. 年龄阶段分布
   6. 新老占比
   7. 地域分布
   中间建了用户连续活跃区间的中间记录表 也就是 字段是区间起始时间和结束时间 
   然后用来做增量的拉链表  节省空间  
   地域也能做拉链表  这样就能记住每次用户地域的变化

   连续活跃报表
   沉默天数报表

3. **留存分析主题**
   字段 计算日期 首访日期 留存天数 留存人数
   日留存率报表
   周留存率报表
4. **漏斗分析主题**
   1. 推送成单漏斗
   2. 暑期促销成单漏斗
   利用正则表达式来判断用户的行为事件序列的模式是否匹配漏斗模型定义中的要求
   rlike 满足正则
   regexp_extract 从字符串提取正则框定的部分
   regexp_repalce 正则替换 
   漏斗分析
   转化率
   点击转化率 预定转化率 支付转化率 
5. **归因主题**
   1. 广告
   2. 价格
   3. 展示界面
   4. 外观
   5. 环境
   6. 地理位置
   用到了spark sql + spark core 混编
      1. 先找出做过目标事件的用户
      2. 再从事件明细表中过滤出这些用户所发生的模型关心的事件记录
      3. 将事件记录数据按照用户分组
      4. 对每个用户的事件序列,按目标事件为分界点,切分成多段 因为一个用户可能会做过多次目标事件
      5. 对每一段按照归因策略算法打分



### 你们事实表包含哪些维度和度量 
维度：地域商圈 客群分类 房源描述 住客点评
度量  
Occ客房入住率 
ADR已售客房平均房价 
RevPAR 可供出租客房的收入 
Revenue月收入
CTP 营业利润贡献
GSS 宾客满意程度指数
MPI 市场渗透指数 

浏览量
点击量
CTR 点击量/浏览量

### 拉链表有什么
拉链表多说几个表 用户活跃 用户订单 用户会员等级


### 什么是拉链表
拉链表就是对我们收到这个信息的一个历史变动然后我们进行处理的形式嘛,也就是保留了历史的一些状态,还有新增的变化数据.
最新数据为开链数据，历史数据称为闭链数据。

### 拉链表的使用场景
1. 表中的部分字段会被更新
2. 需要查看某一个时间点或者时间段的历史信息(例如某个用户在某一个历史时间点的手机号码)
3. 变化比例不是很大的(例如1000万用户总共有每天发生变化的10多万左右)


### 拉链表维护计算的基本逻辑
拉链表一个时间维度中同一个用户只保存一条用户状态。拉链表通常会增加三个“开始日期starttime、结束日期endtime、状态标识mark”。我们就用当前数据通过主键与历史数据进行对比，判断当前数据与历史数据是否发生变化，如果发生变化就进行相应的开链、闭链操作。

### 你们拉链表有哪些表
订单表 用户会员等级表 用户信息表 用户ip所在地域表


### 拉链表补充

- 查询性能：随着时间的推移，拉链表会越来越大，查询性能也可能会有所下降
  保留部分历史数据，比如说一张表里存放着全量的拉链表数据，我们只对外提供一张近3个月数据的拉链表。
- 使用拉链表的时候可以不加end_date，但是加上之后，能优化很多查询；
- 可以加上当前行状态标识，能快速定位到当前状态；
- 在拉链表的设计中可以加一些内容，因为我们每天保存一个状态，如果我们在这个状态里面加一个字段，比如如当天修改次数，那么拉链表的作用就会更大。



### 缓慢变化维有几种 
1. 第一种方式是直接覆盖原值
2. 第二种方式是用代理键去区分历史数据和新增数据
3. 第三种方式是添加新的字段去区别历史和新增


### 做过最难的指标
流量主题下的页面给予下游的贡献量
用sql把头皮抠出来都想不出 
然后去网上借鉴,去问问大佬,就知道可以用算法来实现
大致的步骤就是构建一个二叉树去便利每个页面的子页面,也就是用到了树的后序层次遍历
封装好方法写成dataframe 注册成UDF写spark-sql实现



### 数仓每天跑多少表 
150来张吧 
业务数据40-50张左右 
datax 20分钟左右
每天1点执行 五个小时跑完


### 测试环境
和生产环境电脑配置一样的 三台 

### 怎么保证sql的正确性
就是ADS的报表和mysqL的结果比对一下
或者就是抽取生产环境的2000条数据然后运算核实一下

测试完之后打包脚本 然后跟总监和运维说一下,运维负责上线.


###  你了解的业务域的分析报表: 做过哪些报表  指标有什么 维度什么
业务域我不是很了解,我主要做的是行为域的数据,但同事叫我拉过去帮他们写过sql,自己也稍微了解过一些
大致的内容就是我们会有很多张业务表要进行历史数据的勘察嘛,我们用的工具是DataX,
**对于那些不怎么变化的表,比如字典表啊,地域表啊,积分规则表那种写死了的表,我们就会全量导入到ODS层嘛,**
**而那些数据会不断变化或者是新增数据的表,比如订单表,会员信息表,优惠券领取表这种我们就会每日增量去导入,**
然后将这张表做成拉链表的形式放到dwd层,一般用的星型模型去建立报表,如果统计需求过多我们会提前弄一张宽表存到dws层.
然后我最近做过有关订单的报表,度量总客单价啊,实付金额,优惠金额,
维度有那些性别,地域,会员等级,支付方式等
