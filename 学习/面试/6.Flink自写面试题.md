
### Flink的四大基石
1. Checkpoint重启策略  Checkpoint 本质就是对状态的备份, 持久化到文件系统

2. 状态 用来保存中间计算结果或者缓存数据
旧版 
MemoryStateBackend  基于内存存储
FsStateBackend  基于文件存储
RocksDBStateBackend  基于RocksDB数据库存储 
新版
状态后端有两种存储 (底层的具体实现)
HashMapStateBackend  存什么就是什么类型的 
RocksDbStateBackend  以序列化的形式存储在rocksDB数据库中 
主要在于效率问题 hashMapStateBackend快  因为放在内存中 多出来的溢出在磁盘
弊端为内存空间有限,所以在磁盘的会慢 所以数据量不算大的时候用这个

状态类型有
托管状态和原生状态
然后分成 KeyedState 和 OperatorState 

1. Time 时间语义 

2. Window 将无限的数据流划分成多个有限数据流的手段 希望将数据攒起来做微批次的处理 
事件时间的度量标志为水位线


### watermark的意义
1. 标识Flink任务的事件时间进度，从而推动事件时间窗口的触发和计算
2. 解决事件时间窗口的乱序问题


### watermark的策略有哪些
1. 使用固定延时策略生成水位线，调用WatermarkStrategy中的静态方法forBoundedOutOfOrderness。
2. 使用单调递增的方式生成水位线，调用WatermarkStrategy中的静态方法forMonotonousTimestamps
3. 使用event时间的获取
4. 处理空闲数据源

### Flink时间语义
Flink在1.12版本后默认使用Event Time
1. 处理时间（Process Time）系统时间
2. 事件时间（Event Time）数据源提供的时间
3. 注入时间（Ingestion Time）数据进入flink的时间


### 如何保证数据不丢失
Flink不能保证数据不会丢失，最多只能通过设置较大的延迟时间来让乱序的事件到达。


### Flink保证Exactly Once语义
举例kafka保证Exactly Once
1. source阶段 把kafka消费者的消费位移记录在算子状态中
2. sink阶段
   1. 采用幂等写入方式
   2. 采用两阶段提交事务写入方式
   3. 采用预写日志2PC提交方式



### Flink容错机制
重启策略 本质就是对状态的备份, 持久化到文件系统

一个为checkpoint，一个为savepoint，checkpoint默认为自动保存到状态后端。

### flink checkpoint的原理 以及怎么实现的
Flink分布式快照算法,异步 barrier 快照

### 检查点分界线 barrrier  分布式快照算法
形象化的来说，jobmanager会给每个任务做完的时候插上木板拦截下来让他分身拍照，另一份数据会继续往下流。
通过插入序列号单调递增的barrier，
把无界数据流划分成逻辑上的数据段，
并通过段落标记来为这段数据的处理加持事务的特性，
就有了每一段数据流要么全部一次处理成功，要么回滚一切不完整的任务。
### checkpoint原理描述
jobmanager内部有一个checkpoint协调器,触发之后向TaskManager中的subtask发送RPC消息,让subtask中持有的状态保存到HDFS中或者别的地方(StateBackEnd 保存状态的存储后端),(存储的文件记录了元信息),然后所有TaskManager返回通知给jobmanager,此次checkpoint才算已经完成,如果中途有个出现异常,jobmanager会将此次checkpoint全部取消



### 有没有遇到过checkpoint失败
1. 我们一般回去web ui界面的任务日志去看看错误详情
2. 之前遇到过两个 一个是因为数据倾斜所导致的，我们就将key加随机数打散就ok了
3. 第二个是 我们设置了重启次数嘛，然后还是一直报错，我们就给checkpoint增加并行度增加超时时间，一点用都没有
   1. 然后我们就炸开算子链一个一个的排查
   2. 后来发现我们需要去mysql查询这个数据是否满足条件，由于表很大的缘故里面的sql又写的很差导致长时间没办法返回数据
   3. 就修改sql就好了



### Flink八大分区策略
1. GlobalPartitioner 该分区策略将上游的分发到下游算子的第一个实例中
2. forwordPartitioner 该分区策略用于在同一个operatorChain(算子链)中进行转发,上下游并行度需要一样
3. shuffle partitionner 该分区策略会将元素进行随机分区
4. rebalance partitioner 该分区策略以轮询的方式去为元素分区
5. rescale partitionner 该分区策略会根据上下游的Task数量进行分区,根据节点轮询
6. broadcast partitioner 该分区策略会广播给所有分区,每个节点各有一份复制在自己手里
7. keyGroupSteam Partitioner 该分区策略根据keyGroup的hash进行分区
8. 用户自定义分区 需要用户实现partition接口来实现自己的逻辑


### Flink提交模式
session模式 多个job共享同一个集群，job退出了集群不会退出
（大量小job的时候比较适合，因为不用频繁向yarn注册应用）
提交job先启集群 再提job


perjob模式 每个job独享一个集群 job退出集群也退出 main方法在服务端上运行
（大job运行时长很长的时候比较适合）
提交job的命令即是集群的又是job的

application模式 生产用得多 每个Job独享一个集群， job退出集群也退出 main方法在集群上运行
提交job的命令即是集群的又是job的（底层会使得main在集群上启动）



### Flink提交job的流程
1. 客户端向 ResourceManager 提交 Job， ResouceManager 接到请
求后， 先分配好容器， 然后通知 NodeManager 启动
ApplicationMaster。
3. ApplicationMaster 会加载 HDFS 的配置， 启动对应的
JobManager， 然后 JobManager 会分析当前的作业图， 将它转化成执行图
，从而知道当前需要的具体资源。
4.  接着， JobManager 会向 ResourceManager 申请资源，ResouceManager 接到请求后， 继续分配 container 资源， 然后通知ApplictaionMaster 启动更多的 TaskManager。
5. TaskManager 启动后，会向 JobManager 发送心跳。JobManager 向 TaskManager 分配任务。



### 你们公司Flink用什么监控的 
普罗米修斯+Grafana顾罗方娜  检测Flink的度量Mertic 
有时候也会去flinkweb端页面去看


### 1 小时的滚动窗口,一小时处理一次的压力比较大,想让他5分钟处理一次.怎么办?
自定义触发器，重写4个方法



### window 后面跟 aggregate 和 process 的两个窗口计算的区别是什么？
1. aggregate： 是增量聚合， 来一条数据计算完了存储在累加器中，
不需要等到窗口触发时计算；
2. process： 全量函数， 缓存全部窗口内的数据， 满足窗口触发条件
再触发计算， 同时还提供定时触发， 窗口信息等上下文信息；
3. 应用场景： aggregate 一个一个处理的聚合结果向后传递一般来
说都是有信息损失的， 而 process 则可以更加定制化的处理。



### Flink旁路缓存 
也就是所有请求优先访问Redis缓存，若缓存命中，直接获得数据返回给请求者。如果未命中则查询Hbase数据库，获取结果后，将其返回并写入缓存以备后续请求使用。


### flink 常用算子
map reduce flatmap keyby window broadcast union join connect 



### flink内存管理
堆内内存
网络缓冲区：这个是在TaskManager启动的时候分配的，这是一组用于缓存网络数据的内存，每个块是32K，默认分配2048个
内存池：用于运行时的算法（Sort/Join/Shuffle等），这部分启动的时候就会分配。默认情况下，占堆内存的70% 的大小。
用户使用内存: 这部分的内存是留给用户代码使用的。

Flink堆外内存
堆外内存,用来执行一些IO操作




### Flink 需要划分Task的情况
Flink划分Task主要有四种情况：

（1）类似keyBy，broadcast，rebalance等算子产生shuffer

（2）Parallelism（并行度）变化

（3）new chain，即在算子上执行startNewChain()后，该算子与前面执行的算子分开。

（4）disableChaining，在算子上执行disableChaining()，即算子的开始到结束，单独生成一个task。使用场景，比如该算子逻辑复杂，让算子独自使用一个task内的SubTask。


### Flink 并行度 slot taskmanager的关系
taskmanager 包含多个槽  所以并行度最好为 slot*taskmanager  
并行度如果设置成一个 那九个槽只能用一个槽 其余八个不动      


### Flink反压 



### Flink双流Join
1. window join 也就是将无界数据流变成有界数据流，窗口时间到就触发，触发的内容取决于是inner join 还是outer join
2. interval join 两条流的数据从无界变有界，也就是说A流可以joinB流的时间范围，而window join是两个流的时间join，但如果是outerjoin，那么需要数据需要等待另一条的区间结束了才能得到。一般用于去评估两条流的延迟时间。
3. regular join 无界数据流尝试去关联 因为底层是回撤流，关联不上的先下发，等到能关联上就关联，所以完全保障了数据的时效性。（只能sql实现）
   1. 需要的是sink组件要支持回撤，比如mysql，如果传入的是kafka的话，消费Kafka的也要支持回撤。

常见问题就是状态会变大
我们解决的方法一般都是让flink任务轻量化，安排一个redis作为状态后端，相同key的数据维护在一个hashmap里


### Flink维表join问题
1. 小的维表我们就广播这个表
2. 又或者是用open方法存在内存里，这种就适用维表更新不频繁的
3. 或者是将维度数据导入到redis中，通过异步IO的方式去进行查询。
4. Temporal join 也就是将维度数据映射为一个虚拟表，用主流去join这个虚拟表 
   1. (比如我们公司有一个汇率订单表，每个订单都有不同货币的价格。为了将此表正确地规范化为单一货币，每个订单都需要与下订单时的适当货币兑换率相结合。)


### 数据延迟或者乱序的问题
1. watermark上设置延迟时间
2. 用那个允许迟到的算子allowedlateness,做更后一步的推迟等待
3. 实在不行就设置测流接迟到更久的数据做处理方式然后跟主流后续累加。




### Flink 数据倾斜
1. 首先我们会通过flink的webUI查看相同task的subtask的数据量的情况,看是否出现某些subtask
中的数据量明显大于其他subtask的数量;同时看任务节点的反压情况

2.  我们会出现kafka的topic分区数大于程序的并行度,导致有些subtask中对接多个分区,有些subtask对接的分区数比较少,这时候我们就需要使用数据分发策略相关的算子
keyby之前的数据倾斜:使用shuffle,rebalance,rescale等算子将数据尽量均匀分配
keyby之后的数据倾斜:两阶段聚合:第一个阶段:key拼接随机数前缀或者后缀,将数据分散,然后进行keyby,开窗聚合;第二个阶段:去掉随机数,按照原来的key进行keyby,聚合 (要结合定时器将各个窗口区分开来)