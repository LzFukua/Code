###1.项目背景和项目意义
在公司中则应相对统一，以方便做好公司统一的数据接入规范、统一的数据管理机制、统一的数据处理能力等，做好数据管控。需求一般就是物的数据,人的数据,行为的数据 

1.物的数据
我们早在08年就开始筹建罗盘字典，当时我们是以接到小区单元门楼盘以及户型图之类的这些多维度信息进行描述，我们的房子到目前为止已经承载了两亿多的房屋信息

2人的数据
基本上就是买家，实际上就是买房子的人，还有租户以及业主，经纪人，以及后续我们升级为贝壳品牌，还有介入了品牌以及我们后续的装修人员这些数据

3.行为数据
买家在我们平台上进行的线上浏览行为，以及经纪人会在线下带买家去业主家去看房行为，等等线上和线下的信息



###2.项目的基本框架





###3.用到的技术栈 
* 数据采集系统:Flume(采集行为日志) sqoop(采集业务数)
* 数据存储:Mysql,HDFS,HBASE
* 数据计算 Hive,Spark
* 资源调度系统:Yarn
* 快速查询:Presto
* 数据看台:SuperSet
* 任务调度:Dolphin
* 元数据管理:Atlas


###4.离线数仓项目 , 主要的核心工作有哪些




###5.离线数仓项目 , 意义





###6. 关于flume
>核心角色是agent,相当于一个传输通道,里面有三个组件
>sources 负责读取数据
>channel 缓存读取出来的数据
>sink  写出数据

>内置拦截器
>主机拦截器/静态拦截器

>内置sources
>tailDir/

>内置channel
>memory 和 file 

>内置 sink  
>HDFS

>自定义拦截器的开发步骤 , 完成需求
> 我所认为的拦截器就像一个交通指挥官,把不同种类的日志分发到不同的分析系统,其主要的点在更改flume的event的header的value值

>自定义首先得导入依赖,然后创建一个自定义的拦截器类去实现Interceptor接口,重写里面的四个方法
>1.初始化拦截器
>2.处理每条事件
>3.处理每批事件
>4.结束拦截器
>然后构建一个拦截器的构建器,因为自定义拦截器的类无法直接new，需要通过flume配置文件调用静态内部类，来间接地调用自定义的拦截器对象。
>打一个有依赖的包传入到flume的lib文件下,配置interceptor type 为包名就可以实现了



###7.你们工作时使用的是什么source  channel, 为什么使用他们  
平时我们工作上使用的source是TailDir,可以监控多个目录嘛,而且也有position机制能让我实时监控数据的变化
和断点续传的功能,对于channel 我们使用的是File channel,因为他可以保证数据彻底的安全,且还可以加checkpoint机制加个副本,只是速度不是那么快而已




###8.flume的事务了解吗? 说一下flume的事务!  事务分类 , 事务的流程 ,事务效果 !
了解的,首先flume事务他的就是为了保证数据的可靠传递嘛,他分成了两个数据传递的事务处理,一个是put事务,一个是take事务,我们首先来说说put事务
1. put事务是source端向channel端去传递数据的时候触发的,source会采集数据封装为event,调用doput方法将这批event写到putlist临时缓存区,写满了就进行docommit去检查channel中是否有足够空间去装这批数据,如果可以,putlist就会清空这批数据到channel里,但如果channels容量不足,就进行rollback回滚的操作,清空putlist,然后返回异常给sources,重新采集一遍这批数据进来.

2. 而take事务就是sink端从channel端拉取数据的时候触发的,首先事务开始时调用doTake方法将channel里的event剪切到takeList中,同时也会拷贝一份放到HDFS的IO缓冲流中,当takeList满了之后就会触发doCommit方法,如果数据全部发送成功就会去调用IO流的flush方法将数据写道HDFS磁盘上,同时清空takelist的数据,但如果失败了,就会进行回滚操作,让takelist里的数据重新放入到channel.但这时候可能会导致数据重复,是因为当IO缓存区有一些数据已经传输成功了但回滚操作是把整个takelist的数据都回滚回去重新拉取.

而事务有三个比较重要的参数 一个是sources和sink端的batchSize 以及channel端的capacity 和transactionCapacity  capacity > transactionCapacity > batchSize




###9.什么是flume的failover机制!
在谈failover之前,得先说到级联的问题,级联是在实际生产中我们的日志服务器和HDFS集群之间是不能直接访问的
所以我们会设立中间层的agent去做通信来保证数据的安全,使用的是avro通信,而failover就是上游agent会配置多个sink到不同agent中来实现高可用,避免一个agent宕机导致数据积压在上游.failover会设置了权重比去安排主从的agent传输通道,权重比大的为主节点,而当主节点宕机就开始启用备用节点的通道去传输,在此基础上还会设立退避时间去一段时间询问主节点有没有恢复,如果恢复了就继续让主节点传输.


###10.flume的内部5大组件!
source/channels/sinks/选择器/拦截器



###11.说说flume的优化!
1.设置事务能保证数据不会漏采,但会重复而已
2.设置级联是为了保证了数据的安全,因为在生产过程中是不可以去对日志采集服务器和HDFS服务器进行访问的
3.设置failover是为了保证级联的高可用,防止数据积压在日志采集服务器上
4.在下游服务器sink可以设置 压缩 多线程 的方法去优化传输
5.还有可以用选择器去实现负载均衡,多sink输出到hdfs上



###12.日志采集系统的优化(好像和11差不多)




###13.日志采集系统数据积压问题总结
带宽方面我们可以轻松应对150m/s的数据传输,连峰值都能应对
而且配置了failover策略,避免上游的的积压.




###14.数据量问题 业务特征 规模 用户规模  计算 (时长, 事件频次, 日志数据大小 ,用户数据)
链家 7000万用户  300-400万活跃度 
差不多10多秒产生一个数据 
一个人一次事件产生1.2kb
每个活跃用户平均产生70-80条数据 
每个人一天的事件大小85kb左右
每天280-350多G  
平均公司 按最大的来算就3.5M/s的数据
高峰期35M/s 
(只是暂时的 还需要更改)



###15.数据延迟 ,数据丢失 , 数据重复,数据积压
**数据采集延迟**:
我们首先是有自定义拦截器,抽取日志事件的时间戳,就会保证数据落到正确的文件夹
但比如app上报延迟,或者是网络延迟等情况导致数据没能在第二天计算的时间入仓
所以我们的策略一般是把这个计算时间往后推了一个小时,而且会在三点设置一个脚本去检查昨天的行数以及入仓的行数来进行比对,如果超过了延迟阈值就打算重新采集,但三点之后还没来完的数据咱就不要了

**数据丢失**:按目前的情况来说我们数据是不会丢失的,因为在flume有了多重保证,我们的source是TailDir,这个会有position偏移量所以能保证不会丢,Filechannel里有checkpoint机制双重保证原始数据,比如flume设置了事务出错了会回滚重新采集,最多也就是会数据重复而已

**数据重复**:我们确实会有数据重复的现象
首先我们有web端的页面可以供我们查看hdfs上文件的行数是否和各个服务器采集的条数是否匹配一致,如果出现了多了一大批的现象,我们有两种策略吧,第一种是在入库前就进行脚本去重或者是编写程序去重,第二种是将这个计算压力给到后面的数仓,我们采用的是第一种

**数据积压**:
1. 在业务高峰期会有瞬间的大量日志生成,源头flume没有及时采集会出现积压,所以已经添加了日志服务器去分担了.
2. 然后配置了faliover组去形成高可用,避免因传输不下去导致上游的channel数据积压.
3. 而且我们是用的富裕配置,带宽150M/s,且添加了下游的agent,所以可以随意承接高峰期.
4. 落地HDFS-sink写出的速度和采集速度不匹配,所以采用了多sink的方式去负载均衡
5. 也能适当增加吞吐量





###16.数据服务总线   为什么开发这个服务
这个服务主要是是能给数仓提供了统计每个服务器日志的行数以及flume采集到的HDFS日志的行数,主要的功能有日志上报和日志条数查询

1. 日志的上报具体为通过请求服务接口来传递参数,将每天的日志条数,日志类型,日期,以及服务器名等等上报到服务端,然后将数据通过Mybatis框架存储到Mysql中,且会发送邮件来提示是否已经上传成功

2. 日志的查询功能具体就是调用服务端的接口的来从Mysql中查询日志行数和HDFS的的文件行数来供后续的去重脚本使用

###17.数据去重流程
1. 脚本会去请求服务端调用查询的方法得到原始数据行数和HDFS的行数,如果HDFS上的数据大于原始数据的行数阈值就进行去重操作






2. 去重流程为首先我们在外创建一张外部表,分区为日期,然后将原始的日志数据映射到表里
3. 然后我们会用hive sql 的distinct或者group by 进行去重操作,再把去重后的数据压缩覆盖原始的数据
4. 我们还会根据要求用spark core 或者spark sql进行去重操作 但我们脚本目前还是用的hive


###18.数据入库(表有结构)流程 
在进行了去重的操作后会直接将数据导入到ODS层,步骤是先创建一张按日期分区的外部表,提取原始数据的json格式映射到表中,且会对表判断是否需要去重,如果需要去重就将去重后的数据压缩覆盖掉原来的数据,如果不需要去重就将原始数据直接导入ODS层,入库的结果会通过邮件的形式发送


###19.数据预处理  工作
因为原始数据都会掺杂着脏数据,主要的点会有核心字段数据丢失,数据格式错误等,且对于业务会有会对数据有不同的处理方式
在预处理分了四个步骤 
1. 数据清洗,对一些异常的数据根据文档需求进行删除\替换\插补的方式
2. 数据集成,对同名异义或者异名同义的进行合并
3. 数据变换,就是对数据进行规范化,满足软件或者分析的需求
4. 数据规约,就是对数据本身内容的理解基础上,寻找依赖于发现目标的数据有用的特征,最大限度的精简数据量.


###20.全局Guid的实现
我们是以动态绑定的策略去实现的,也就是拿行为数据表去关联用户信息表,获取用户的账户作为数据的唯一标记,
但是如果这条事件没有账户,我们就去关联(设备与账号绑定权重表),获取此设备一个最大权重的账号guid,如果这个设备从来没有过账号,我们就设计一个临时的guid作为表示,后续如果登了账号,就用那个账号作为唯一标识

###21.数据从ODS层到DWD层的预处理流程
1. **清洗过滤**：我们会对数据进行一个清洗过滤，主要是过滤掉一些废弃字段、关键字段缺失的记录，再将清洗后json格式的数据打平，解析成ORC格式。
2. **session分割**:为了判断此次会话时间是否是正常的.首先按用户分组以时间排名,取当前记录与下一行的时间求差值,时间戳差值大于30分钟,就标记为1,否则为0,然后对组内进行累加,将session与sum值进行拼接,生成一个新的字段new_session作为该次会话判断的依据
3. **数据规范化**: 在session分割后,我们根据需求文档进行数据规范处理,比如说统一日期字段等
4. **地理位置集成**:本质上做了个维度退化,减少后续的join运算,以用户行为日志的经纬度转成geohash,然后去对应公司的mysql字典表,如果地理位置有就加载字典表里的,如果没有就请求网络API得到地理位置再补充到表中和mysql的字典表中.
5. **唯一Guid标识**:为了提高用户行为分析的准确,我们给每个用户的访问记录打赏一个全局的唯一标识,我们采用的是动态绑定策略,创建了**用户设备权重表**,**无账号的设备临时guid表**
    1. 如果用户的行为数据有账号,就去关联用户的信息表,把用户uid作为唯一标识
    2. 如果用户没有账号,就去关联设备账号权重表,去得到该设备权重最大的账号uid
    3. 如果这台设备从未绑定过账号,我们就回临时生成一个guid作为它的唯一标识 :guid(nvl(max(guid) , 500000000))
6. **创建DWD明细表**:
   1. 用户信息表去与用户行为表(也就是已经清洗过滤,session分割,数据规范化后的表),where条件为账户不为空,即以他的账户作为guid
   2. 然后union 用户行为表用户栏为空的与用户设备权重表和无账号的设备临时guid表关联,取权重最大的用户为空用户的guid+如果用户设备权重表没有此设备nvl(t2.account_id , t3.tmp_id),则无账号的设备临时guid作为他的guid






###22.数仓分层,为什么要分层
数仓分了四--五层(大层为四层 ODS -- DW --ADS  )
1. **ODS**层--原始数据层:存放原始数据
ODS层是最接近数据源的一层,主要的目的为简化后续数据加工处理的工作.
1. **DWD**层--数据明细层:对ODS层数据进行清洗脏数据,维度退化,脱敏,规范化字段等.
在DOS的基础上对数据进行加工处理,提供更干净的数据(维度退化例如订单id,这种量级很大的维度,没必要用一张维度表来进行存储,但订单id在数据分析又非常重要,所以我们将订单id冗余在事实表中,这种维度就是退化维度)
3. **DWS**层--数据汇总层:对DWD层数据将进行一个轻度的汇总
DWS层会轻度汇总,粒度会比明细表数据稍粗,会针对度量值进行汇总,避免重复计算.该层的数据表相对比较少,大多数为宽表(一张表会涵盖较多的业务内容,表中的字段较多).用于提供后续的业务查询,减少重复开发.
4. DIM层--维表层(根据公司业务看是否分了这一层):如果维度表过多,则可能会单独分出来这一层
主要包含了两部分数据
高基数维度数据:一般是用户资料表,商品资料表,数据量千万上亿级别
低基数维度数据:一般是配置表,比如日期维表,数据量个位数到万左右.
5. **ADS**层--数据服务层/应用层:提供数据产品和数据分析使用的数据
这里的数据就是给顶层的应用程序消费使用,对接外部系统,方便前端业务快读查询.

1. 分层是一种空间换时间的操作,可以减少重复计算带来的效率低问题
2. 把复杂的任务分解多层完成,方便定位问题
3. 减少重复开发,规范化数据,像图书馆一样列好,还能保证数据安全,使得数据血缘更清晰,数据表和指标统一规划

###关于主题
主题:根据统计需求,将具有相似统计特征的内容放在一个分组上,方便后续的计算维护管理等
1. **流量主题**
   1. 页面分析
   2. 会话分析
   3. 用户分析
2. **访客主题**
   1. 会话次数
   2. 访问时长
   3. 连续访问天数
   4. 留存分析
   5. 性别比例
   6. 年龄阶段分布
   7. 新老占比
   8. 地域分布
3. **漏斗主题**
   1. 广告成单漏斗
   2. 暑期促销成单漏斗
4. **归因主题**
   1. 广告
   2. 价格
   3. 性能
   4. 外观
5. 业务主题(不建议说)

###你们做的流量主题的受访页面分析/报表,核心的维度是什么,核心的度量是什么?
我们做的这个主题下核心的度量比如有浏览量(pv),访客量(uv),为下游的贡献量,平均停留时长等
核心的维度有,近三天,近七天,新老访客等

