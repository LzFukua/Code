###1.项目背景和项目意义
在公司中则应相对统一，以方便做好公司统一的数据接入规范、统一的数据管理机制、统一的数据处理能力等，做好数据管控。需求一般就是物的数据,人的数据,行为的数据 

1.物的数据
我们早在08年就开始筹建罗盘字典，当时我们是以接到小区单元门楼盘以及户型图之类的这些多维度信息进行描述，我们的房子到目前为止已经承载了两亿多的房屋信息

2人的数据
基本上就是买家，实际上就是买房子的人，还有租户以及业主，经纪人，以及后续我们升级为贝壳品牌，还有介入了品牌以及我们后续的装修人员这些数据

3.行为数据
买家在我们平台上进行的线上浏览行为，以及经纪人会在线下带买家去业主家去看房行为，等等线上和线下的信息



###2.项目的基本框架





###3.用到的技术栈 
* 数据采集系统:Flume(采集行为日志) sqoop(采集业务数)
* 数据存储:Mysql,HDFS,HBASE
* 数据计算 Hive,Spark
* 资源调度系统:Yarn
* 快速查询:Presto
* 数据看台:SuperSet
* 任务调度:Dolphin
* 元数据管理:Atlas


###4.离线数仓项目 , 主要的核心工作有哪些




###5.离线数仓项目 , 意义





###6. 关于flume
>核心角色是agent,相当于一个传输通道,里面有三个组件
>sources 负责读取数据
>channel 缓存读取出来的数据
>sink  写出数据

>内置拦截器
>主机拦截器/静态拦截器

>内置sources
>tailDir/

>内置channel
>memory 和 file 

>内置 sink  
>HDFS

>自定义拦截器的开发步骤 , 完成需求
> 我所认为的拦截器就像一个交通指挥官,把不同种类的日志分发到不同的分析系统,其主要的点在更改flume的event的header的value值

>自定义首先得导入依赖,然后创建一个自定义的拦截器类去实现Interceptor接口,重写里面的四个方法
>1.初始化拦截器
>2.处理每条事件
>3.处理每批事件
>4.结束拦截器
>然后构建一个拦截器的构建器,因为自定义拦截器的类无法直接new，需要通过flume配置文件调用静态内部类，来间接地调用自定义的拦截器对象。
>打一个有依赖的包传入到flume的lib文件下,配置interceptor type 为包名就可以实现了



###7.你们工作时使用的是什么source  channel, 为什么使用他们  
平时我们工作上使用的source是TailDir,可以监控多个目录嘛,而且也有position机制能让我实时监控数据的变化
和断点续传的功能,对于channel 我们使用的是File channel,因为他可以保证数据彻底的安全,且还可以加checkpoint机制加个副本,只是速度不是那么快而已




###8.flume的事务了解吗? 说一下flume的事务!  事务分类 , 事务的流程 ,事务效果 !
了解的,首先flume事务他的就是为了保证数据的可靠传递嘛,他分成了两个数据传递的事务处理,一个是put事务,一个是take事务,我们首先来说说put事务
1. put事务是source端向channel端去传递数据的时候触发的,source会采集数据封装为event,调用doput方法将这批event写到putlist临时缓存区,写满了就进行docommit去检查channel中是否有足够空间去装这批数据,如果可以,putlist就会清空这批数据到channel里,但如果channels容量不足,就进行rollback回滚的操作,清空putlist,然后返回异常给sources,重新采集一遍这批数据进来.

2. 而take事务就是sink端从channel端拉取数据的时候触发的,首先事务开始时调用doTake方法将channel里的event剪切到takeList中,同时也会拷贝一份放到HDFS的IO缓冲流中,当takeList满了之后就会触发doCommit方法,如果数据全部发送成功就会去调用IO流的flush方法将数据写道HDFS磁盘上,同时清空takelist的数据,但如果失败了,就会进行回滚操作,让takelist里的数据重新放入到channel.但这时候可能会导致数据重复,是因为当IO缓存区有一些数据已经传输成功了但回滚操作是把整个takelist的数据都回滚回去重新拉取.

而事务有三个比较重要的参数 一个是sources和sink端的batchSize 以及channel端的capacity 和transactionCapacity  capacity > transactionCapacity > batchSize




###9.什么是flume的failover机制!
在谈failover之前,得先说到级联的问题,级联是在实际生产中我们的日志服务器和HDFS集群之间是不能直接访问的
所以我们会设立中间层的agent去做通信来保证数据的安全,使用的是avro通信,而failover就是上游agent会配置多个sink到不同agent中来实现高可用,避免一个agent宕机导致数据积压在上游.failover会设置了权重比去安排主从的agent传输通道,权重比大的为主节点,而当主节点宕机就开始启用备用节点的通道去传输,在此基础上还会设立退避时间去一段时间询问主节点有没有恢复,如果恢复了就继续让主节点传输.


###10.flume的内部5大组件!
source/channels/sinks/选择器/拦截器



###11.说说flume的优化!
1.设置事务能保证数据不会漏采,但会重复而已
2.设置级联是为了保证了数据的安全,因为在生产过程中是不可以去对日志采集服务器和HDFS服务器进行访问的
3.设置failover是为了保证级联的高可用,防止数据积压在日志采集服务器上
4.在下游服务器sink可以设置 压缩 多线程 的方法去优化传输
5.还有可以用选择器去实现负载均衡,多sink输出到hdfs上



###12.日志采集系统的优化




###13.日志采集系统数据积压问题总结
带宽方面我们可以轻松应对150m/s的数据传输,连峰值都能应对
而且配置了failover策略,避免上游的的积压.




###14.数据量问题 业务特征 规模 用户规模  计算 (时长, 事件频次, 日志数据大小 ,用户数据)
链家 7000万用户  300-400万活跃度 
差不多10多秒产生一个数据 
一个人一次事件产生1.2kb
每个活跃用户平均产生70-80条数据 
每个人一天的事件大小85kb左右
每天280-350多G  
平均公司 按最大的来算就3.5M/s的数据
高峰期35M/s 
(只是暂时的 还需要更改)



###15.数据延迟 ,数据丢失 , 数据重复 
**数据采集延迟**:原因是比如app上报延迟,或者是网络延迟等情况导致数据没能在第二天计算的时间入仓
所以我们的策略一般是把这个计算时间往后推了一个小时,而且会在三点设置一个脚本去检查昨天的行数以及入仓的行数来进行比对,如果超过了延迟阈值就打算重新采集,但三点之后还没来完的数据咱就不要了

**数据丢失**:按目前的情况来说我们数据是不会丢失的,因为在flume有了多重保证,比如Filechannel里有checkpoint机制,比如flume设置了事务出错了会回滚重新采集,最多也就是会数据重复而已

**数据重复**:我们确实会有数据重复的现象,首先我们有web端的页面可以供我们查看hdfs上文件的行数是否和各个服务器采集的条数是否匹配一致,如果出现了多了一大批的现象,我们有两种策略吧,第一种是在入库前就进行脚本去重或者是编写程序去重,第二种是将这个计算压力给到后面的数仓,我们采用的是第一种