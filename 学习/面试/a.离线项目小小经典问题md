
### 数据去重流程
1. 脚本会去请求服务端调用查询的方法得到原始数据行数和HDFS的行数,如果HDFS上的数据大于原始数据的行数阈值就进行去重操作
2. 去重流程为首先我们在外创建一张外部表,分区为日期,然后将原始的日志数据映射到表里
3. 然后我们会用hive sql 的distinct或者group by 进行去重操作,再把去重后的数据压缩覆盖原始的数据
4. 我们还会根据要求用spark core 或者spark sql进行去重操作 但我们脚本目前还是用的hive


### 数据入库(表有结构)流程 
在进行了去重的操作后会直接将数据导入到ODS层,步骤是先创建一张按日期分区的外部表,提取原始数据的json格式映射到表中,且会对表判断是否需要去重,如果需要去重就将去重后的数据压缩覆盖掉原来的数据,如果不需要去重就将原始数据直接导入ODS层,入库的结果会通过邮件的形式发送


### 数组源里面的脏数据你们是怎么处理的
我们对于核心字段缺失的就直接过滤掉了,然后只是游客登录的空账户信息,我们会做一个全局的Guid去处理这个(然后blabla说怎么处理的guid)


### 全局Guid的实现
我们是以动态绑定的策略去实现的,也就是拿行为数据表去关联用户信息表,获取用户的账户作为数据的唯一标记,
但是如果这条事件没有账户,我们就去关联(设备与账号绑定权重表),获取此设备一个最大权重的账号guid,如果这个设备从来没有过账号,我们就设计一个临时的guid作为表示,后续如果登了账号,就用那个账号作为唯一标识

### 数据从ODS层到DWD层的预处理流程
1. **清洗过滤**：我们会对数据进行一个清洗过滤，主要是过滤掉一些废弃字段、做一些关键字段缺失的补充，然后把清洗后的json格式的数据打平，解析成ORC格式。
2. **session分割**:为了判断此次会话时间是否是正常的.首先按用户分组以时间排名,取当前记录与下一行的时间求差值,时间戳差值大于30分钟,就标记为1,否则为0,然后对组内进行累加,将session与sum值进行拼接,生成一个新的字段new_session作为该次会话判断的依据
3. **数据规范化**: 在session分割后,我们根据需求文档进行数据规范处理,比如说统一日期字段等
4. **地理位置集成**:本质上做了个维度退化,减少后续的join运算,以用户行为日志的经纬度转成geohash,然后去对应公司的mysql字典表,如果地理位置有就加载字典表里的,如果没有就请求网络API得到地理位置再补充到表中和mysql的字典表中.
5. **唯一Guid标识**:为了提高用户行为分析的准确,我们给每个用户的访问记录打赏一个全局的唯一标识,我们采用的是动态绑定策略,创建了**用户设备权重表**,**无账号的设备临时guid表**
    1. 如果用户的行为数据有账号,就去关联用户的信息表,把用户uid作为唯一标识
    2. 如果用户没有账号,就去关联设备账号权重表,去得到该设备权重最大的账号uid
    3. 如果这台设备从未绑定过账号,我们就会临时生成一个guid作为它的唯一标识 :guid(nvl(max(guid) , 500000000))
6. **创建DWD明细表**:
   1. 用户信息表去与用户行为表(也就是已经清洗过滤,session分割,数据规范化后的表),where条件为账户不为空,即以他的账户作为guid
   2. 然后union 用户行为表用户栏为空的与用户设备权重表和无账号的设备临时guid表关联,取权重最大的用户为空用户的guid+如果用户设备权重表没有此设备nvl(t2.account_id , t3.tmp_id),则无账号的设备临时guid作为他的guid



###  你了解的业务域的分析报表: 做过哪些报表  指标有什么 维度什么
业务域我不是很了解,我主要做的是行为域的数据,但同事叫我拉过去帮他们写过sql,自己也稍微了解过一些
大致的内容就是我们会有很多张业务表要进行历史数据的勘察嘛,我们用的工具是sqoop,对于那些不怎么变化的表,比如字典表啊,地域表啊,积分规则表那种写死了的表,我们就会全量导入到ODS层嘛,而那些数据会不断变化或者是新增数据的表,比如订单表,会员信息表,优惠券领取表这种我们就会每日增量去导入,然后将这张表做成拉链表的形式放到dwd层,一般用的雪花模型去建立报表,如果统计需求过多我们会提前弄一张宽表存到dws层.
然后我最近做过有关订单的报表,度量总客单价啊,实付金额,优惠金额,维度有那些性别,地域,会员等级,支付方式等


