### 海量日志数据，提取出某日访问百度次数最多的那个IP
既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序：

首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hashmap对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP.

### 寻找热门查询，300万个查询字符串中统计最热门的10个查询
因为这个可以都装进内存里,所以只需要hashmap + 堆

1. hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashTable，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计；

2. 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为NlogK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O(N)+N∗O(logK)，（N为1000万，N为300万）。


### 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词
文件很大+内存受限制 
分而治之 + hash统计 + 堆/快速排序

1. 顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。

2. 对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。

3. 取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。

--- 


### 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10
如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素：

1. 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。
2. 分布式：求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。

但如果同一个元素重复出现在不同的电脑中呢？这个时候，你可以有两种方法：

遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。




### 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想
如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。


### 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解
、首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。


### 100w个数中找出最大的100个数
1. 采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。

2. 采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w∗100)。

3. 在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w∗lg100)。


### 100万个数字的文件乱序排列,使用何种算法进行排序
希尔排序 具体思路是先拆分成若干个小的 然后再排


### 获取大于10GB的超大文本文件的最后10行
这个我好像在哪看到过，就是用java的IO包里面有个reverselinesFileReader方法

### 大数据如何找中位数
采用基于二进制比较和快排的分割思想 
首先是分批去将数据读到文件里面，每个数字都用二进制表示，然后比他们的最高位也就是32位，如果数字最高位是0，那它就是负数，就去0号文件，如果最高位是1，那它就去1号文件，然后比如0号文件有2亿个数字，1号文件有8亿个数字，那么就知道中位数一定在1号文件排序后的3亿的位置，那么就不要0号文件，对1号文件下手，然后比较31位，分出来的1.0号文件和1.1号文件就可得知1.1的文件一定会比1.0号文件大，然后一样的看两边文件有多少。再继续对30位分文件，一直不断循环到内存可以快排为止。


### 海量数据TopK问题
假设这样一个场景，一个问题阅读量越高，说明这个问题越有价值，越应该推送给用户

假设数据量有1亿，取Top100
1. 方法是分治法，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的100个(即每份数据的TopK)，最后在剩下的100*100个数据里面找出最大的100个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的100个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于100个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于100个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于100个，就在小的那堆里面快速排序一次，找第100-n大的数字；递归以上过程，就可以找到第100大的数。参考上面的找出第100大数字，就可以类似的方法找到前100大数字了。
   
2. Hash法。如果这1亿个数里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的100个数。
   
3. 采用最小堆。首先读入前100个数来创建大小为100的最小堆，然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有100个数字。建堆时间复杂度是O(m)，堆调整的时间复杂度是O(logm)，最终的时间复杂度=1次建堆的时间+n次堆调整的时间，所以该算法的时间复杂度为O(nlogm)，空间复杂度是100（常数）。



### 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据
上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。


### 海量数据找出不重复的整数
在2.5亿个正整数中找出不重复的整数。

思路一：
分治法 + HashMap 
将 2.5 亿个整数，分批操作，例如分成 250 万一批，共100批次。每批使用循环遍历一次，存入 HashMap<int1,int2> 里面，int1 对应这个数，int2 对应它出现的次数，没出现就默认是 1 次。每操作完一批，就进行当前的 HashMap 的去重操作，读出 int2 > 1 的，排除掉。接下来的批次，以此类推，得出 100，剩下的自然就是不重复的。

### 海量数据找出重复次数最多的
我们把海量数据很多份嘛，然后使用 HashMap<int,int> 来统计。然后每份里面找出最大的前100个数，然后全部加起来，继续分治处理，或者直接进行排序，排序算法可以选快排等之类的，前100个就是结果。