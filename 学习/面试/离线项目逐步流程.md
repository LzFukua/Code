###1.flume采集日志
一开始我们用日志模拟器去生成了模拟日志 出了100个用户以及一堆的行为日志,行为日志记录为json格式
   

fLume具有**分布式**,**可靠**,**高可用**,可以采集文件的工具
flume最核心的角色为agent,每一个agent相当于一条数据(被封装成Event)传递员
内部有三个核心组件 **source**,**channel**,**Sink**

Event 数据在agent内部流转的封装形式 Source获取到原始组件之后,封装成Event放入Channal,Sink从Channal抽出Event后根据目标存储的需求转成其他输出格式
Event主要由两部分组成 Headers 和 Body , Header是一个Map集合 


事务机制 (put take 操作) 涉及的三个重要参数
- a1.sources.s1.batchSize =100 
- a1.sinks.k1.batchSize = 200  
- a1.channels.c1.transactionCapacity = 300 （应该大于source或者sink的batchSize）
上面channals这个要区别于  a1.channels.c1.capacity = 10000 

拦截器:内置拦截器-->主机\时间
自定义拦截器

级联 + failover + 选择器  

数据积压   数据峰值  数据延迟  数据重复  数据丢失

---

###入库去重 
spring boot 项目
POST请求,调用接口得到每天每台服务器的日志条数,HDFS日志条数收集到mysql,写脚本判断是否需要去重(创表一个字段line group by 或者sparksql sparkcore),发送邮箱 (日志上报功能,日志查询功能)

首次需要创建外部表,解析gz格式下的json,映射表,分区入库


###预处理
1. session 分割 : 一个session中两个时间的间隔>30 分钟 算两次会话
    **spark-core**
   1. session不为null
   2. 对session进行分组,以时间排序
   3. 一开始为0,每次大于30分组后面+1


    **spark-sql**
   1. 对session分组排序,如果上一个减下一个大于30,输出1
   2. 拼接session , sum()over()  这样每个会话结束都为1 每个会话开始为0 sum()over() 累加  
    session-0  
    session-0  
    session-1
    session-2
    session-2
    新建字段 得到数据

2. 集成地理位置
   1. 公司有坐标表在mysql,用GEOHash算法进行地理位置匹配
   2. 大致流程为,从本地库根据经纬度获取地理位置信息,如果本地库没有的话,就去网络API查询,然后将查到的数据传回本地库中(可以在表里建立唯一key这样就不会重复了,或者有别的办法)
   3. 

