sqoop不能生成orc文件
生成 parquetfile 

但parquetfile在hive比orc性能差一些

1.增量导入 按条件导入   --query 'select...  AND $CONDITIONS' 


增量适合数据变化较少的数据 


全量导入的是 那些几乎不会发生变化的字段 (地域 id 字典等)
增量导入导的是 会缓慢变化的字段 (订单状态 发货状态等)



业务表 百来张  我也不清楚

第一范式 每一列都是不可分割的原子数据项 

第二范式 消除非主键属性对主键的部分依赖
在一范式的基础上 每个字段都必须依赖主键   不允许存在部份依赖   
比如一个表里有姓名 系名  系主任 分数 课程   分数必须由课程加学号决定  放在一张表  

第三范式  消除传递依赖
任何非主属性不依赖于其他非主属性  
系名 系主任 不应该和学生挂钩    所以应该一个主键类型一张表
消除数据的冗余  提高数据的一致性

业务最重要的是事务 和 一致性 

主要维度 时段  会员等级  支付方式 订单来源  促销活动 地域等


雪花模型的维表按范式建成的

什么叫维度模型 

先设立一张事实表,围绕着事实表进行去找所需要计算的维度表

维度建模的理解
1. 维度建模常用于数仓建模 做数据分析使用而不是事务处理
2. 先设立一张事实表,围绕着事实表的度量进行去找所需要计算的维度表进行计算,将维度表进行关联形成宽表进行报表计算

星型模型 雪花模型  星座模型

**你了解的业务域的分析报表: 做过哪些报表  指标有什么 维度什么**
业务域我不是很了解,我主要做的是行为域的数据,但同事叫我拉过去帮他们写过sql,自己也稍微了解过一些
大致的内容就是我们会有很多张业务表要进行历史数据的勘察嘛,我们用的工具是sqoop,对于那些不怎么变化的表,比如字典表啊,地域表啊,积分规则表那种写死了的表,我们就会全量导入到ODS层嘛,而那些数据会不断变化或者是新增数据的表,比如订单表,会员信息表,优惠券领取表这种我们就会每日增量去导入,然后将这张表做成拉链表的形式放到dwd层,一般用的雪花模型去建立报表,如果统计需求过多我们会提前弄一张宽表存到dws层.
然后我最近做过有关订单的报表,度量总客单价啊,实付金额,优惠金额,维度有那些性别,地域,会员等级,支付方式等