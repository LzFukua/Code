clickhouse 分布式比较差  维护起来很麻烦 但单机很强
doris 单机比clickhouse差一点,但分布式弥补了他的缺点 还有使用会简单 社区也更活跃

最新的数据 和 时效性报表的需求 
实时是辅助线 
不过，随着公司业务的发展，公司的app运营推广部，市场营销部等部门开始对决策数据产生了一些高时效性的需求（运营活动的实时流量分析和订单分析);

行为日志
Sink 一个到HDFS  一个到Kafka 然后让flink接到kafka -- doris

得到时效型的报表 
某活动参与人数

业务数据
cdc 读 业务库  然后用连接器往doris插入 并不能支持高并发

OLAP doris 
主体是离线数仓  即席查询  就不用presto了  直接用Doris

这种叫MPP库 

不查也得算的 实时大屏看板 流计算处理 
固定的核心指标 走流式计算

灵活计算 有人来查才计算的  预处理直接到doris  即席查询



----
实时监控用户行为  针对人群推送特定的消息  任意定规则 受众条件的即时计算   动态规则 规则可变参数可变

画像条件的逻辑分析  直接从画像里面筛人群
行为事件类型以及属性   判断某个用户的行为事件是否是按照规则条件要求
行为序列的判断 用户行为是否满足规则条件依次发生 有规则以及时间要求  (也可以加次数或者是属性值匹配,也可以是对比关系)


分词搜索能力的 ES solr 但ES并发度有上限  需要更多的服务器 人群圈选用ES  毫秒级别
所以为了省钱 用户画像标签先根据规则圈选出来 然后注入到flink  把人群ID用Bitmap注入
1. 一个计算时去查ES(实时生产画像)
2. 规则上线之前把目标人群圈选出来 形成bitmap注入到flink (画像是离线生产的)

滚动增量计算 (大的逻辑方案) 

即席查询的规则  如果横跨了历史和未来 就用两个

groovy 脚本语言 实现具体计算逻辑  编译后.class文件 加载后反射回给类中去拿到别的类的数据  连通信息注入到flink,动态的编译加载得到对象然后调用计算方法
高内聚 低耦合

每一类规则条件的计算 都有一个规则运算状态机 每个状态机都封装了自己的计算逻辑,也负责维护自己的数据结构和逻辑

- 搜索兴趣词包含“商务休闲”的用户，如果浏览包含“商务休闲”的商品页面，则触发一个优惠券推送消息；

画像系统是离线的  不是实时变更的  所以新人来的时候他没有以前的标签 但可能因为点点点触发了规则导致他还是会出现推送或者别的

规则上线前的历史数据查询结果传递给规则引擎,规则引擎将结果和后续的数据运算逻辑进行接续整合(把历史数据作为未来查询运算的初始值)
而历史已经满足规则的人,就直接整合到满足人群的固化数据去

flink计算的中间结果 (状态) 存储到redis中去 因为redis读写数据很快 能满足高并发低延迟的读写需求 且有丰富的数据结构支持 还对外开放

事务型数据库 
分析型数据库 



----
