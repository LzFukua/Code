能不能不用打  hdfs dfs -ls hdfs://hadoop001:8020/ 后面这一串
取决于 /opt/apps/hadoop-3.1.1/etc/hadoop/core-site.xml 是否设置了 
fs.defaultFS  value为hdfs://hadoop001:8020
以及环境变量有没有设置这个conf路径
即spark同理

提交程序 spark://hadoop001:7077 
hdfs://hadoop001:8020
web端 8080为spark 8088为yarn 9870为HDFS 
hive 9083 metastore
    10000 hiveserver2
zookeeper 2181 连接端口 

spark-submit 
--master spark://hadoop001:7077 必须
--deploy-mode client或者是 cluster 必须
--class 包名  必须
--name 程序名 必须

--jars 第三方jar包

(以虚拟机为4G内存4核基础)
--driver-memory 1G 提供1G内存给Driver端 (比如广播变量)
--executor-memory 2G  不设置的话默认1G
--executor-cores 1G  不设置的话默认全占用

如果设置了 那就会使得一个机器上有两个Executor 但如果内存受限(因为内存只能使用大概百分之70左右也就是2.7G,所以分不了两个2G2C的)


client模式下 xx.jar来自于本地  
cluster模式下 xx.jar得在HDFS上

spark-submit脚本-->调用spark-class脚本-->调用有main方法的一个类进行干活(根据用户传入的参数进行反射创建类的实例,就能得到方法,或者做初始化的信息)-->SparkContext(上下文对象,运行环境)-->创建以下东西 -- Driver\DAGScheduler(DAG生成器,Stage划分)\TaskScheduler(任务调度)\ClinetEndPoint(监控打印收集)\SchedulerBackEnd(不停的运行)
(接着Driver端开始做事)
通过DAGScheduler的行动算子去解析代码,根据创键DAG划分Stage(划分逻辑等),然后创建TaskSet,生成Task,接着让TaskScheduler去调度任务给Worker,在任务shuffer阶段会有落盘情况,那ResultTask就会去拿数据,输出结果

Yarn基本组成
Resource Manager
ResourceScheduler 
ApplicationsMaster ASM
NodeManager
ApplicationMaster

    